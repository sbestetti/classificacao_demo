{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo de análise preditiva com Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas necessárias\n",
    "\n",
    "Esse é o `requirements.txt` utilizado no projeto:\n",
    "\n",
    "    asttokens==2.4.1\n",
    "    attrs==23.2.0\n",
    "    comm==0.2.2\n",
    "    contourpy==1.2.0\n",
    "    cycler==0.12.1\n",
    "    debugpy==1.8.1\n",
    "    decorator==5.1.1\n",
    "    exceptiongroup==1.2.0\n",
    "    executing==2.0.1\n",
    "    fastjsonschema==2.19.1\n",
    "    fonttools==4.49.0\n",
    "    ipykernel==6.29.3\n",
    "    ipython==8.22.2\n",
    "    jedi==0.19.1\n",
    "    joblib==1.3.2\n",
    "    jsonschema==4.21.1\n",
    "    jsonschema-specifications==2023.12.1\n",
    "    jupyter_client==8.6.1\n",
    "    jupyter_core==5.7.2\n",
    "    kiwisolver==1.4.5\n",
    "    matplotlib==3.8.3\n",
    "    matplotlib-inline==0.1.6\n",
    "    nbformat==5.10.2\n",
    "    nest-asyncio==1.6.0\n",
    "    numpy==1.26.4\n",
    "    packaging==24.0\n",
    "    pandas==2.2.1\n",
    "    parso==0.8.3\n",
    "    pexpect==4.9.0\n",
    "    pillow==10.2.0\n",
    "    platformdirs==4.2.0\n",
    "    plotly==5.19.0\n",
    "    prompt-toolkit==3.0.43\n",
    "    psutil==5.9.8\n",
    "    ptyprocess==0.7.0\n",
    "    pure-eval==0.2.2\n",
    "    Pygments==2.17.2\n",
    "    pyparsing==3.1.2\n",
    "    python-dateutil==2.9.0.post0\n",
    "    pytz==2024.1\n",
    "    pyzmq==25.1.2\n",
    "    referencing==0.33.0\n",
    "    rpds-py==0.18.0\n",
    "    scikit-learn==1.4.1.post1\n",
    "    scipy==1.12.0\n",
    "    six==1.16.0\n",
    "    stack-data==0.6.3\n",
    "    tenacity==8.2.3\n",
    "    threadpoolctl==3.3.0\n",
    "    tornado==6.4\n",
    "    traitlets==5.14.2\n",
    "    tzdata==2024.1\n",
    "    wcwidth==0.2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação do dataset\n",
    "\n",
    "Aqui criamos um dataframe Pandas com o conteúdo do arquivo csv. O dataframe é a estrutura padrão de manejo de dados para bibliotecas de ML em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv(\"churn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração dos dados\n",
    "\n",
    "Uma vez que o dataset tenha sido carregado, podemos começar uma análise exploratória, para entendermos a estrutura e os tipos dos nossos dados, e tentar auferir possíveis co-relações. Essa exploração também pode ser feita antes da importação dos dados, utilizando ferramentas de BI como planilhas, PowerBI e etc. Mas também temos algumas funções interessantes para essa exploração dentro do próprio Python.\n",
    "\n",
    "O primeiro passo é verificar a qualidade dos dados, certificando que tenhamos o mínimo possível de células sem informação. Para isso, podemos utilizar a função `info()` do dataset, que nos dá uma visão resumida dos dados, incluindo os tipos de colunas e a quantidade de dados nulos, caso haja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A função `head()` mostra o cabeçalho e os 5 primeiros registro do dataset e é muito útil para entendermos a estrutura dos dados. É um ótimo ponto de partida para visualizarmos claramente quais informações estão disponíveis e o que estamos tentando categorizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotando gráficos\n",
    "\n",
    "Outra ferramenta interessante é a possibilidade de plotar gráficos que representem nossos dados, para fácil vizualização.\n",
    "\n",
    "Um dos tipos mais úteis de gráfico é o histograma, que é ótimo para propriedades qualitativas. Podemos, por exemplo, criar e exibir um histograma co-relacionando o país de nosso clientes e a taxa de churn assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(dataset, x = \"churn\", text_auto = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(dataset, x = \"pais\", text_auto = True, color = \"churn\", barmode = \"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro tipo de gráfico bem últil é o BoxPlot, ideal para valores numéricos, pois mostra muitas informações de uma só vez, inclusive outliers. Podemos plotar um gráfico Box com os salários dos clientes assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(dataset, x=\"salario_estimado\", color=\"churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "\n",
    "Com os dados compreendidos, podemos iniciar o processo de limpeza e transformação desses dados.\n",
    "\n",
    "### Limpeza e separação\n",
    "Vamos começar apagando a coluna *id_cliente* que não vai ser utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(\"id_cliente\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então separamos o dataset em dois conjuntos:\n",
    "- **x**: dataset contendo todas as colunas de dados que serão utilizadas para treinar o modelo\n",
    "- **y**: dataset contendo apenas a variável **churn**, que é o que queremos predizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[\"churn\"]\n",
    "x = dataset.drop(\"churn\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui nós salvamos os nomes das colunas do dataset para uso futuro, pois quando o dataset for transformado para o processo de treinamento do modelo, essa informação será perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_colunas = x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação\n",
    "\n",
    "Uma vez que os datasets tenham sido limpos e separados, precisamos tranformar os dados categóricos em numéricos, para que o modelo possa classificá-los.\n",
    "\n",
    "Como pretendemos utilizar o modelo de **Árvore de Decisão**, vamos utilizar um codificador chamado **OneHotEncoder**. O que ele faz é criar novas colunas para cada categoria textual no dataframe, marcar a coluna correspondente com o valor 1 e as colunas restantes com 0. Também vamos configurá-lo para, caso a coluna só tenha dois valores, como `sexo_biologico`, ele mantém uma coluna para apenas um dos valores e marca o registro com 1 caso positivo e 0 caso negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(drop=\"if_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O codificador criado no passo anterior será agora utilizado por uma **função de transformação**. Essa é a função que efetivamente aplicará as mudanças no dataset, de acordo com as configurações do codificado criado anteriormente. Vamos criar uma instância dessa função, e como parâmetros vamos passar:\n",
    "- Uma tupla contendo:\n",
    "    - O codificador\n",
    "    - A lista de nomes de colunas a serem transformadas\n",
    "- Uma instrução do que fazer com os demais campos (nesse caso, nada)\n",
    "- Quanto de limite de sparsing aplicar. Devemos manter em 0 para modelos de Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_column_transformer(\n",
    "    (\n",
    "        encoder,\n",
    "        [\n",
    "            \"pais\",\n",
    "            \"sexo_biologico\"\n",
    "        ]\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    sparse_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos nosso transformer iniciado e configurado com o nosso codificador, podemos aplicá-lo ao dataset de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transformer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objeto retornado pela função de transformação é um Array NumPy. Vamos transformá-lo de volta em um Dataframe Pandas. Vamos também utilizar a função `get_feature_names_out()` do transformador com a lista de nomes de colunas que salvamos no começo da demo para dar nomes úteis às colunas do Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pandas.DataFrame(x, columns=transformer.get_feature_names_out(nomes_colunas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos checar o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando o dataset alvo\n",
    "\n",
    "No nosso caso específico, a coluna *churn* já está em valor numérico. Mas caso ela não estivese, teríamos que passar o nosso dataset objetivo (y) por uma função transformadora também. Isso pode ser feito com a classe `LabelEncoder`. Essa classe só deve ser utilizada para transformar o dataset alvo, já que o resultado é um array de categorias. Caso você passe o dataset alvo pelo LabelEncoder, lembre-se de que você não precisa transformar ele num Dataframe Pandas. O modelo pode utilizar o dataset de alvo como um array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividindo o dataset em dados para treino e dados para teste\n",
    "\n",
    "Depois de explorar e transformar os dados conforme nossa necessidade, só nos resta dividir nosso dataset em dois datasets separados. Um deles, o maior, será utilizado para treinar o modelo. O segundo, um pouco menor, será utilizado para testar o modelo. É importante que o modelo nunca tenha *visto* os dados de teste, para podermos ter certeza de que ele não apenas decorou os dados.\n",
    "\n",
    "A biblioteca de modelos do pacote SciKit tem uma função própria para fazer essa divisão: `train_test_split()`. Vamos executá-la passando como parâmetros os dois datasets (x de treino e y de objetivo) e qual é o dataset objetivo com a propriedade `stratify`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando e testando modelos\n",
    "\n",
    "Com os datasets transformados e separados podemos finalmente começar a trabalhar nos modelos de predição.\n",
    "\n",
    "### Modelo Dummy\n",
    "\n",
    "Vamos começar setando um parâmetro mínimo de qualidade para os nossos modelos. Esse mínimo vai ser setado por um modelo chamado **Dummy**. O que esse modelo faz é analisar os valores que tentamos predizer e simplesmente escolher sempre o valor mais prevalente, assim *\"acertando\"* na maioria dos casos.\n",
    "\n",
    "Para utilizar o model, instânciamos a classe `DummyClassifier` e então *inserimos* os datasets transformados de dados e respostas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(x_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso Dataset de 10 mil registros, 7963 registros tem um valor positivo para churn. O modelo deve identificar que a maioria dos valores é positivo e então usar positivo em todos os testes. Então, se aplicarmos o modelo ao Dataset, deveríamos ter uma taxa de acerto de aproximadamente **79.63%**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.score(x_teste, y_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Árvore de Decisão\n",
    "\n",
    "Com o baseline de aproximadamente 79% definido, já temos como avaliar a qualidade de um modelo real. Vamos importar o modelo `DecisionTreeClassifier` e aplicá-lo aos nossos dados, de maneira similar com o que fizemos com o modelo Dummy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore = DecisionTreeClassifier()\n",
    "arvore.fit(x_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos avaliar a qualidade das previsões, lembrando que o modelo Dummy acertou 79%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore.score(x_teste, y_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajuste do modelo\n",
    "\n",
    "Apesar do resultado ser melhor do que do modelo Dummy, não foi por uma margem expressiva. Para melhorar isso, podemos ajustar diversos parâmetros do modelo. Mas podemos começar a análise desenhando a árvore de decisão resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,10))\n",
    "plot_tree(arvore, filled=True, fontsize=7, class_names=[\"nao\", \"sim\"], feature_names=x_treino.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que a árvore ficou bem grande e confusa. Vamos limitar o tamanho da árvore e checar se o resultado melhora. Para isso utilizamos um parâmetro chamado `max_depth`, que define o máximo de escolhas que a árvore pode fazer antes de *tomar uma decisão*. Vamos re-instanciar a árvore, mas dessa vez setando o `max_depth` em 15, re-aplicar ao dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore = DecisionTreeClassifier(max_depth=15)\n",
    "arvore.fit(x_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... e verificar o score novamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore.score(x_teste, y_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que houve uma melhora. Nesse caso, poderíamos testar diferentes valores até encontrar o que retorna o melhor resultado, o que pode ser bem trabalhoso, ou utilizar a classe `RandomizedSearchCV` para testar possíveis valores automaticamente.\n",
    "\n",
    "Para utilizar a classe `RandomizedSearchCV` precisamos antes de um dicionário com os parâmetros para os quais vamos buscar o melhor resultado e o range de valores a tentar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribution = {\n",
    "    \"max_depth\": randint(1, 15),\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"splitter\": [\"best\", \"random\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o dicionário pronto, vamos instanciar a classe passando o tipo de modelo para o qual queremos buscar os melhores parâmetros (**DecisionTreeCalssifier** nesse caso), quantas iterações o buscador deve fazer e, finalmente, o dicionário com os parâmetros a serem testados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_iter=50,\n",
    "    param_distributions=param_distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a instância gerada, vamos aplicar os dados de treino de deixar ela fazer sua mágica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(x_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado é um objeto do tipo **estimator**, que tem entre seus parâmetros o campo `best_param_`, um dicionário com a informação que buscamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos re-configurar nossa árvore com o melhor `max_depth` possível..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore = DecisionTreeClassifier(max_depth=5, criterion=\"gini\", splitter=\"best\")\n",
    "arvore.fit(x_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... e testar novamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore.score(x_teste, y_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver como ficou nossa árvore definitiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,10))\n",
    "plot_tree(arvore, filled=True, fontsize=7, class_names=[\"nao\", \"sim\"], feature_names=x_treino.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o modelo criado, treinado e otimizado, podemos finalmente utilizá-lo para predizer um dado. Vamos começar criando um dicionário que represente uma possível nova linha no nosso dataset e então transformá-lo em um dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo_dado = {\n",
    "    \"score_credito\": [619],\n",
    "    \"pais\": [\"França\"],\n",
    "    \"sexo_biologico\": [\"Mulher\"],\n",
    "    \"idade\": [42],\n",
    "    \"anos_de_cliente\": [2],\n",
    "    \"saldo\": [0.00],\n",
    "    \"servicos_adquiridos\": [1],\n",
    "    \"tem_cartao_credito\": [1],\n",
    "    \"membro_ativo\": [1],\n",
    "    \"salario_estimado\": [101348.88]\n",
    "}\n",
    "\n",
    "novo_dado = pandas.DataFrame(novo_dado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois disso, precisamos aplicar a mesma função de transformação utilizada para treinar o modelo e converter o resultado novamente em um dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo_dado = transformer.transform(novo_dado)\n",
    "novo_dado = pandas.DataFrame(novo_dado, columns=transformer.get_feature_names_out(nomes_colunas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E estamos finalmente prontos para pedir que nosso modelo tente predizer um dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvore.predict(novo_dado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
